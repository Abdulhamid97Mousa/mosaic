# LLM Worker Dependencies (vLLM)
# Install with: pip install -r requirements/llm_worker.txt
#
# This file contains dependencies for LLM-powered agents that use
# vLLM for local GPU inference (like BALROG-style agentic LLM evaluation).
#
# vLLM provides OpenAI-compatible API for local model serving.
# See: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html

# Include base requirements
-r base.txt

# ═══════════════════════════════════════════════════════════════
# vLLM Server (Local GPU Inference)
# ═══════════════════════════════════════════════════════════════

# HTTP client for vLLM OpenAI-compatible API
requests>=2.31.0

# Async HTTP client (for concurrent requests)
httpx>=0.27.0

# vLLM for local GPU inference
# See: https://docs.vllm.ai/en/latest/
# Note: Requires NVIDIA GPU with CUDA support
vllm>=0.6.0

# ═══════════════════════════════════════════════════════════════
# vLLM Server Setup
# ═══════════════════════════════════════════════════════════════
#
# Start vLLM server with a HuggingFace model:
#
#   vllm serve meta-llama/Llama-3.1-8B-Instruct \
#     --host 0.0.0.0 \
#     --port 8000 \
#     --api-key token-abc123
#
# With GPU memory optimization:
#
#   vllm serve mistralai/Mistral-7B-Instruct-v0.2 \
#     --dtype float16 \
#     --gpu-memory-utilization 0.9
#
# ═══════════════════════════════════════════════════════════════
# Configuration
# ═══════════════════════════════════════════════════════════════
#
# Set in .env:
#   VLLM_BASE_URL=http://localhost:8000/v1
#   VLLM_API_KEY=token-abc123  (or EMPTY for no auth)
#
# ═══════════════════════════════════════════════════════════════
# PettingZoo Integration (multi-agent environments)
# ═══════════════════════════════════════════════════════════════

# Note: PettingZoo can also be installed via requirements/pettingzoo.txt
# for full environment support. This includes minimal deps for LLM agents.
pettingzoo>=1.24.0

# ═══════════════════════════════════════════════════════════════
# Optional: Install vLLM itself (requires CUDA)
# ═══════════════════════════════════════════════════════════════
#
# vLLM is typically installed separately as it requires CUDA:
#   pip install vllm
#
# Or with specific CUDA version:
#   pip install vllm --extra-index-url https://download.pytorch.org/whl/cu118
