# Worker Telemetry Integration Guide

## Quick Start: Integrating TelemetryGrpcProxy into SPADE-BDI Worker

### Prerequisites
1. Trainer daemon running: `python -m gym_gui.services.trainer_daemon`
2. Worker has access to `gym_gui.services.trainer.trainer_telemetry_proxy`

### Minimal Integration Pattern

```python
# Inside spadeBDI_RL_refactored/worker.py or core/runtime.py

import asyncio
import json
import time
from gym_gui.services.trainer.trainer_telemetry_proxy import TelemetryGrpcProxy

async def run_episodes_with_telemetry(run_id: str, agent_id: str, episodes: int):
    """Run episodes and stream telemetry to daemon."""
    
    async with TelemetryGrpcProxy(target="127.0.0.1:50055").session() as tx:
        for ep_idx in range(episodes):
            obs, info = env.reset()
            ep_reward = 0.0
            step_count = 0
            done = False
            
            while not done:
                # Get action from policy
                action = policy.select_action(obs)
                
                # Step environment
                next_obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
                
                # Emit step telemetry
                await tx.emit_step(
                    run_id=run_id,
                    episode_index=ep_idx,
                    step_index=step_count,
                    action_json=json.dumps(int(action)),
                    observation_json=json.dumps(obs.tolist() if hasattr(obs, 'tolist') else obs),
                    reward=float(reward),
                    terminated=terminated,
                    truncated=truncated,
                    policy_label="BDI-Q-learning",
                    backend="SPADE-BDI worker",
                    agent_id=agent_id,
                    render_hint_json=json.dumps(info.get("render_hint")) if "render_hint" in info else None,
                    payload_version=1,
                    timestamp_ns=time.time_ns(),
                )
                
                ep_reward += reward
                step_count += 1
                obs = next_obs
            
            # Emit episode telemetry
            await tx.emit_episode(
                run_id=run_id,
                episode_index=ep_idx,
                total_reward=ep_reward,
                steps=step_count,
                terminated=terminated,
                truncated=truncated,
                metadata={"seed": info.get("seed", 0)},
                agent_id=agent_id,
                timestamp_ns=time.time_ns(),
            )
```

### Integration into HeadlessTrainer

```python
# spadeBDI_RL_refactored/core/runtime.py

class HeadlessTrainer:
    def __init__(self, adapter, config: RunConfig, emitter: TelemetryEmitter):
        self._adapter = adapter
        self._config = config
        self._emitter = emitter
        self._use_grpc = config.extra.get("use_grpc_telemetry", True)
    
    def run(self) -> int:
        """Run training episodes."""
        if self._use_grpc:
            # New path: stream to daemon
            return asyncio.run(self._run_with_grpc())
        else:
            # Legacy path: JSONL to stdout
            return self._run_with_jsonl()
    
    async def _run_with_grpc(self) -> int:
        """Stream telemetry via gRPC."""
        from gym_gui.services.trainer.trainer_telemetry_proxy import TelemetryGrpcProxy
        
        run_id = self._config.run_id
        agent_id = self._config.agent_id
        
        try:
            async with TelemetryGrpcProxy(target="127.0.0.1:50055").session() as tx:
                for ep_idx in range(self._config.episodes):
                    obs, info = self._adapter.reset()
                    ep_reward = 0.0
                    step_count = 0
                    done = False
                    
                    while not done:
                        action = self._adapter.select_action(obs)
                        step_result = self._adapter.step(action)
                        
                        await tx.emit_step(
                            run_id=run_id,
                            episode_index=ep_idx,
                            step_index=step_count,
                            action_json=json.dumps(action),
                            observation_json=json.dumps(step_result.observation),
                            reward=step_result.reward,
                            terminated=step_result.terminated,
                            truncated=step_result.truncated,
                            policy_label=self._config.policy_strategy.value,
                            backend="SPADE-BDI worker",
                            agent_id=agent_id,
                            payload_version=1,
                            timestamp_ns=time.time_ns(),
                        )
                        
                        ep_reward += step_result.reward
                        step_count += 1
                        obs = step_result.observation
                        done = step_result.terminated or step_result.truncated
                    
                    await tx.emit_episode(
                        run_id=run_id,
                        episode_index=ep_idx,
                        total_reward=ep_reward,
                        steps=step_count,
                        terminated=step_result.terminated,
                        truncated=step_result.truncated,
                        metadata={"policy": self._config.policy_strategy.value},
                        agent_id=agent_id,
                        timestamp_ns=time.time_ns(),
                    )
            
            return 0
        except Exception as exc:
            self._emitter.run_completed(run_id, status="failed", reason=str(exc))
            return 1
    
    def _run_with_jsonl(self) -> int:
        """Legacy JSONL output to stdout."""
        # Existing implementation...
        pass
```

### Configuration Toggle

Add to your training config JSON:

```json
{
  "run_id": "run_abc123",
  "env_id": "FrozenLake-v1",
  "agent_id": "bdi_q_agent_1",
  "episodes": 100,
  "extra": {
    "use_grpc_telemetry": true
  }
}
```

### Field Mapping Reference

| Worker Concept | gRPC Field | Type | Notes |
|---|---|---|---|
| Episode number | `episode_index` | int | 0-based index |
| Step number | `step_index` | int | 0-based within episode |
| Action | `action_json` | str | JSON-serialized action |
| Observation | `observation_json` | str | JSON-serialized obs |
| Reward | `reward` | float | Step reward |
| Episode done | `terminated` | bool | Normal termination |
| Truncated | `truncated` | bool | Time/resource limit |
| Policy name | `policy_label` | str | e.g., "BDI-Q-learning" |
| Worker type | `backend` | str | e.g., "SPADE-BDI worker" |
| Agent ID | `agent_id` | str | Unique agent identifier |
| Render metadata | `render_hint_json` | str | Optional JSON render hints |
| Schema version | `payload_version` | int | Set to 1 |
| Timestamp | `timestamp_ns` | int | `time.time_ns()` |

### Error Handling

```python
async def safe_emit_step(tx, **kwargs):
    """Emit step with error recovery."""
    try:
        await tx.emit_step(**kwargs)
    except grpc.aio.AioRpcError as exc:
        if exc.code() == grpc.StatusCode.UNAVAILABLE:
            # Daemon offline, fall back to JSONL
            print(json.dumps({"type": "step", **kwargs}), file=sys.stdout)
        else:
            raise
```

### Connection Options

```python
# Default (localhost)
TelemetryGrpcProxy(target="127.0.0.1:50055")

# Custom daemon address
TelemetryGrpcProxy(target="192.168.1.100:50055")

# Larger message limit (for high-res observations)
TelemetryGrpcProxy(target="127.0.0.1:50055", max_message_bytes=128 * 1024 * 1024)
```

### Testing Workflow

1. **Start daemon**:
   ```bash
   python -m gym_gui.services.trainer_daemon
   ```

2. **Start GUI** (separate terminal):
   ```bash
   python -m gym_gui.app
   ```

3. **Submit worker config** via GUI Train Agent dialog or:
   ```bash
   echo '{"run_id":"test_run","env_id":"FrozenLake-v1","agent_id":"test_agent","episodes":10,"extra":{"use_grpc_telemetry":true}}' | python -m spadeBDI_RL_refactored.worker
   ```

4. **Observe**:
   - Daemon logs show `PublishRunSteps` calls
   - GUI automatically creates `Agent_test_agent_Tab`
   - Tab displays live steps as they stream

### Debugging Tips

**No tab appears:**
- Check daemon is running (`ps aux | grep trainer_daemon`)
- Verify worker connects successfully (check worker logs for gRPC errors)
- Ensure `agent_id` is non-empty in config

**Tab appears but no data:**
- Verify `emit_step()` calls are awaited
- Check daemon logs for telemetry ingestion
- Confirm `run_id` matches between submit and emit calls

**Performance issues:**
- Reduce emit frequency (batch multiple steps)
- Increase hub `max_queue` in bootstrap
- Enable payload compression in future iteration

### Migration Checklist

- [ ] Import `TelemetryGrpcProxy` in worker
- [ ] Wrap episode loop in `async with proxy.session()`
- [ ] Replace JSONL emits with `await tx.emit_step()`
- [ ] Add `await tx.emit_episode()` on episode completion
- [ ] Test with daemon offline (fallback handling)
- [ ] Verify GUI tab creation
- [ ] Confirm step data displays correctly

### Next Steps After Integration

1. Remove legacy JSONL stdout telemetry
2. Add policy artifact upload before training
3. Implement episode replay from daemon store
4. Add real-time metrics aggregation
5. Support multi-agent coordination telemetry

---

**Status**: ðŸ“– Ready for Worker Integration | âœ… Proxy Implementation Complete
