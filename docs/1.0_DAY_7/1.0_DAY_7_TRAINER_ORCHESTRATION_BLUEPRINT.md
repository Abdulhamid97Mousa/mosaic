# Trainer Orchestration Blueprint (Contrarian, Bullet-Proof Edition)

This blueprint operationalizes the trainer orchestrator plan with explicit guardrails, failure-mode mitigations, and delivery gates that we can actually execute against.

## Quick Plan-of-Attack

1. Re-anchor on the north-star intent and non-negotiable guardrails.
2. Define the architecture (services, contracts, and lifecycle) with concrete anti-break patterns baked in.
3. Spell out failure modes and fixes so we know what will break and how to catch it early.
4. Sequence the roadmap with validation gates that prove each guardrail before moving on.
5. Lock in verification and ops drills to keep the system resilient.

## North-Star Intent

- Qt UI stays strictly human-facing; the trainer daemon owns all long-running work.
- Orchestration APIs become the single source of truth for GUI, CLI, and automation clients.
- Telemetry, video artifacts, and metrics flow through the existing storage/observability spine without ad-hoc side channels.

## Non-Negotiable Guardrails

1. Never block the Qt main thread—bridge daemon traffic through a dedicated QThread or external client shim.[^qt-thread]
2. Control-plane sockets stay binary-clean; heavy artifacts ride their own channel so Unix socket buffers never choke.[^unix-buffer]
3. Keep the SQLite run registry’s WAL lean via proactive checkpointing.[^sqlite-wal]
4. Preserve subscriber safety with XPUB/XSUB snapshot + replay semantics for metrics streams (or leverage gRPC streaming if we drop ZeroMQ).[^zeromq-guard]
5. Follow Valkka’s bootstrap order (spawn processes before Qt threads) and message-bridge discipline.[^valkka-qt]
6. Reserve GPUs explicitly per run (CUDA visibility) instead of trusting `torch.cuda.is_available()`.
7. Treat CleanRL/CORL as black-box subprocesses with crisp contracts—no in-process imports.

## Architecture Blueprint

- **Trainer Daemon** (`python -m gym_gui.services.trainer_daemon`): async event loop, gRPC control surface, optional ZeroMQ metrics fan-out, SQLite run registry in WAL mode.
- **Trainer Workers** (`python -m gym_gui.services.trainer_worker --config <path>`): spawn per run, wrap CleanRL/CORL scripts, emit structured stdout/metrics/artifact events, enforce CUDA visibility mask.
- **Clients** (`TrainerClient` service in GUI/CLI/tests): submit configs, stream status/logs, request stop/cancel via gRPC. Qt front-end subscribes through a bridge thread that turns daemon events into queued signals.
- **Storage & Telemetry**: integrate with `TelemetryService`, JSONL recorder, OTLP exporter, and StorageRecorder (ValkkaFS-inspired retention, no blobs in DB).[^valkka-fs]

## Process Lifecycle (with Built-In Safeguards)

1. **Bootstrap**
	- Launch daemon before any Qt threads start.
	- Daemon binds a well-known gRPC endpoint and optional UNIX socket; `bind-or-die` enforces singleton.
	- Schedule periodic WAL checkpoints; reconcile stale runs.
2. **Submission**
	- Validate `TrainRunConfig` against a versioned JSON Schema.
	- Pre-flight GPU/dataset availability; allocate GPU slots before dispatch.
3. **Dispatch**
	- Write config to temp dir; spawn worker in a new process group; record PGID for later signals.
	- Emit `RunStarted`; start draining stdout/stderr streams immediately via asyncio.
4. **Monitoring**
	- Stream logs/metrics with monotonic sequence IDs; heartbeats every second.
	- Throttle log rate and drop excess with counters; keep control channel small (IDs, numbers, URIs).
5. **Termination**
	- Persist `RunCompleted`/`RunFailed`; move artifacts into managed storage; prune temp dirs.
	- Release GPU slot and checkpoint WAL.
6. **Stop**
	- Send `SIGTERM` to process group, wait with timeout, escalate to `SIGKILL`; capture operator intent and exit reason.

## Service Contract (Contrarian Edition)

- **gRPC API**
  - `SubmitRun(TrainRunConfig) → RunHandle`
  - `StreamStatus(RunHandle) → RunEvent` (bidirectional streaming with flow control)
  - `StopRun(RunHandle, reason)`
  - `GetHealth() → DaemonStatus`
- **Run Registry Schema**: `(run_id, status, config_digest, started_at, finished_at, last_heartbeat, artifact_manifest, failure_reason, gpu_slot)` plus WAL checkpoint schedule.
- **Metrics Bus**: XPUB/XSUB proxy enforcing replay of last N events keyed by sequence number (or gRPC streaming fallback).

## Failure Modes & Fixes (Bullet-Proofing Checklist)

| # | Failure Mode | Guardrail / Fix | Validation Hook |
|---|--------------|-----------------|-----------------|
| 1 | Daemon “check-then-spawn” race creates two daemons. | Bind a fixed gRPC/UNIX endpoint on startup; if taken, exit. Maintain lockfile for diagnostics. Workers spawned in their own PGID; TERM→timeout→KILL escalation.[^jmlr-singleton] | CI test launches concurrent health checks; ensure only one daemon survives. |
| 2 | GUI freeze due to blocking IPC. | All GUI↔daemon calls async with timeouts; dedicate `TrainerQBridgeThread` for I/O; marshal results via Qt signals.[^qt-thread] | UI test simulates stalled daemon; verify GUI stays responsive. |
| 3 | Subprocess PIPE deadlocks when stdout/stderr flood. | Use `asyncio.create_subprocess_exec`; stream-read both pipes; never `wait()` from UI thread.[^py-subprocess] | Integration test pushes 10MB/s logs; ensure workers complete without hang. |
| 4 | ZeroMQ slow-joiner drops first metrics. | Provide XPUB/XSUB replay buffer (last N events) or switch to gRPC streaming with built-in flow control.[^zeromq-guard] | Metrics replay test: start subscriber late, assert it receives replay. |
| 5 | Control-plane socket buffers overflow. | Keep control channel message size < 4KB (IDs, numbers, URIs). Ship artifacts/frames by path/object store.[^unix-buffer] | Property-based test ensures payload schema stays within limits. |
| 6 | SQLite WAL grows unchecked / single-writer stalls. | `journal_mode=WAL`, `synchronous=NORMAL`, small transactions, auto-checkpoint every 1000 pages, no blobs in DB.[^sqlite-wal] | Load test submits 100 runs; WAL size capped, writes stay <50ms. |
| 7 | Fork-after-Qt threads hazard. | Daemon runs out-of-process; GUI never forks after Qt threads exist.[^valkka-qt] | Startup smoke test verifies GUI only connects to existing daemon. |
| 8 | GPU contention not prevented by `torch.cuda.is_available()`. | Track GPU slots; set `CUDA_VISIBLE_DEVICES` per worker; deny launches when capacity exhausted.[^nvidia-gpu] | Concurrency test launches >GPU slots; expect queued/denied runs. |
| 9 | CleanRL treated as library, causing shared-state bleed. | Always invoke scripts via subprocess with explicit config; no in-process imports.[^cleanrl-subprocess] | Unit test ensures orchestrator refuses to import CleanRL module. |

## Delivery Roadmap with Exit Criteria

1. **Week 1 – Daemon Skeleton**
	- Implement gRPC service + SQLite registry + schema validation.
	- Prove singleton binding (test concurrent startups) and WAL auto-checkpoint.
2. **Week 2 – Worker Integration**
	- Launch real CleanRL DQN via subprocess; stream stdout/stderr asynchronously; maintain PGID.
	- Validate GPU reservation + release cycle with mocked devices.
3. **Week 3 – Stop/Recovery**
	- Implement TERM→KILL ladder; orphan cleanup; WAL checkpoint background job.
	- GUI/CLI clients receive `RunStarted/RunCompleted` via asynchronous bridge.
4. **Week 4 – Telemetry & Artifacts**
	- Bridge metrics into TelemetryService + JSONL; register MP4 assets via StorageRecorder.
	- Add XPUB/XSUB replay buffer (or migrate metrics to gRPC stream) and verify slow-joiner tests.
5. **Week 5 – Harden & Observe**
	- Ship OTLP traces, Grafana-ready metrics; exercise chaos drills (GPU missing, disk full, stalled subscriber, daemon crash) guided by Valkka debugging/pitfall notes.[^valkka-debug][^valkka-pitfalls]
	- Document runbook and finalize acceptance checklist.

## Validation & Ops Playbook

- **Automated**: CI orchestrates mock runs, concurrent daemon starts, log floods, and GPU contention scenarios; ensures guardrails hold.
- **Manual Drills**: Follow Valkka debugging steps (gdb, `/dev/shm` cleanup) and Qt responsiveness tests during chaos drills.[^valkka-debug]
- **Operational Docs**: Update trainer orchestration guide, GPU allocation policy, and failure response runbooks referencing the local Valkka docs.

## References

[^qt-thread]: Qt Documentation — QThread Class (`references/qt_qthread_doc.md`).
[^unix-buffer]: Linux man-pages Project — `unix(7)` (`references/man7_unix7.md`).
[^sqlite-wal]: SQLite WAL Checkpointing Notes (`references/sqlite_wal_checkpointing.md`).
[^zeromq-guard]: “ZeroMQ: Messaging for Many Applications”, Chapter 5 (`references/zeromq_chapter5_pub_sub.md`).
[^valkka-qt]: Valkka Examples — Integrating with Qt (`references/valkka_qt_notes.md`).
[^valkka-fs]: Valkka Examples — ValkkaFS guide (`references/valkka_valkkafs.md`).
[^valkka-tests]: Valkka Examples — Installing & testsuite (`references/valkka_requirements.md`).
[^valkka-debug]: Valkka Examples — Debugging (`references/valkka_debugging.md`).
[^valkka-pitfalls]: Valkka Examples — Common problems & pitfalls (`references/valkka_pitfalls.md`).
[^jmlr-singleton]: Journal of Machine Learning Research — Process supervision best practices (external reference, not stored locally).
[^py-subprocess]: Python Documentation — `asyncio.create_subprocess_exec` guidance (external reference).
[^nvidia-gpu]: NVIDIA Developer Documentation — GPU process isolation and `CUDA_VISIBLE_DEVICES` (external reference).
[^cleanrl-subprocess]: CleanRL Project Documentation & GitHub guidance on script-based usage (external reference).

