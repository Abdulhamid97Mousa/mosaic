# Day 7 – Trainer Orchestration Development Plan (Contrarian Perspective)

## Executive Intent

> Establish a resilient trainer orchestration layer **before** any new CLI ergonomics. The GUI remains a Qt-first, human-facing shell; long-running training jobs, CleanRL processes, and offline pipelines live in a dedicated service that both the GUI and future automation clients talk to via a shared configuration contract.

### North-Star Outcomes

- **Decouple UI responsiveness from training workloads.** No CleanRL script runs inside the Qt event loop ever again.
- **Single source of truth for configuration.** All knobs (env, seed, overrides, actors, capture settings) feed into one `TrainRunConfig` schema.
- **Symmetric tooling.** GUI and CLI (and tests) consume the same orchestration APIs.
- **Telemetry convergence.** Training metrics, stdout, artifacts, and video references flow into the existing telemetry/storage stack without ad-hoc adapters.

---

## Architecture Blueprint

### Process Topology

```text
+----------------------+                 +---------------------------+
|  gym_gui.app (Qt UI) |  submit/config  |  Trainer Daemon           |
|  - SessionController | <--------------> |  - IPC request queue      |
|  - TelemetryService  |                 |  - Metrics PUB/SUB bus    |
|  - TrainerClient     |                 |  - Run registry           |
+----------------------+                 |  - Worker supervisor      |
                                         +-----------+---------------+
                                                     |
                                                     | spawn worker
                                                     v
                                         +---------------------------+
                                         | Trainer Worker Process    |
                                         |  - Adapter factory        |
                                         |  - CleanRL / CORL scripts |
                                         |  - Metrics emitter        |
                                         +---------------------------+
```

- **Daemon** runs as an async process (anyio/asyncio) with an IPC endpoint (UNIX socket or ZeroMQ) and a PUB/SUB metrics channel.
- **Worker processes** are spawned per run, wrapping CleanRL (or other trainers) as child processes. Workers talk back to the daemon via structured pipes.
- **GUI and CLI clients** communicate only with the daemon on 127.0.0.1 (or configurable host), keeping UI logic thin and deterministic.

### Service Components

| Component | Responsibilities | Notes |
| --- | --- | --- |
| `TrainerClient` (GUI/CLI) | Serialize configs, submit/stop runs, subscribe to events. | Lives in `gym_gui/services/trainer_client.py`. |
| `TrainerDaemon` | Validate jobs, persist registry, supervise workers, broadcast metrics. | `python -m gym_gui.services.trainer_daemon`. |
| `TrainerWorker` | Materialize env via adapters, invoke CleanRL/CORL, stream stdout/metrics/artifacts. | `python -m gym_gui.services.trainer_worker --config <path>`. |
| `RunRegistry` | Durable store of run state (`submitted`, `running`, `success`, `failed`). | SQLite table under `gym_gui/var/training/runs.sqlite`. |
| `MetricsBus` | PUB/SUB fan-out for logs, metrics, status events. | ØMQ, WebSocket, or POSIX message queue. |
| `Telemetry bridge` | Consumes `TrainerEvent`s and writes into `TelemetryService` tables & JSONL. | Enforced by GUI client; CLI can tap in for headless monitoring. |

---

## Configuration Contract (`TrainRunConfig`)

All consumers share the same dataclasses (enforced via JSON Schema). Draft:

```python
@dataclass(slots=True)
class TrainRunConfig:
    run_id: str
    env_id: GameId
    seed: int
    control_mode: ControlMode
    actor_id: str
    overrides: dict[str, Any]
    algorithm: TrainingAlgorithm
    hyperparameters: dict[str, Any]
    storage_profile: str
    telemetry_sinks: list[TelemetrySink]
    video_capture: VideoCaptureProfile
    notes: str | None = None
```

- **`run_id`**: GUID or timestamped slug; created client-side for traceability.
- **`overrides`**: JSON-serializable diff against the per-environment dataclass (e.g., `{"enable_time_penalty": true}` for Taxi).
- **`hyperparameters`**: Flattened dict derived from GUI panels or imported config files.
- **`telemetry_sinks`**: `sqlite`, `jsonl`, `wandb`, `tensorboard`, etc.
- **`video_capture`**: Reuses the RecordVideo plan (`enabled`, `trigger`, `storage_dir`).

**Serialization:** use `msgspec` or `pydantic` for validation; ship JSON schema alongside docs so ops tooling can validate configs without Python.

---

## Trainer Daemon Lifecycle

1. **Startup**
   - Bind request socket and metrics bus.
   - Open `runs.sqlite` in WAL mode.
   - Reconcile stale runs (mark `running` → `aborted` with reason "daemon_restart").

2. **Submission**
   - Receive `TrainRunConfig`, validate (env exists, actor available, GPU if required, disk quota via StorageRecorder profile).
   - Persist row in `RunRegistry` (`submitted`).
   - Queue job; if concurrency limit not exceeded, dispatch immediately.

3. **Dispatch**
   - Create temp directory under `var/training/tmp/<run_id>`.
   - Write config JSON and optional CleanRL CLI config file.
   - Spawn worker via `subprocess.Popen` (detached session, env sanitized).
   - Emit `RunStarted` event.

4. **Monitoring**
   - Receive structured messages from worker (`stdout`, `metrics`, `checkpoint`, `video`, `heartbeat`).
   - Forward events on metrics bus; update `RunRegistry` incremental fields (last metric timestamp, checkpoint path).
   - If heartbeat stalls (timeout), mark run as `failed` with reason `worker_timeout` and send SIGKILL.

5. **Termination**
   - On normal exit, record `status=success`, capture artifacts, emit `RunCompleted`.
   - On error exit, capture `stderr`, exit code, emit `RunFailed` with root cause hints.

6. **Stop request**
   - On `stop(run_id)`, set a cancellation flag; send graceful signal (SIGTERM) to worker. If not respected within timeout, escalate to SIGKILL.

7. **Shutdown**
   - Wait for workers to finish or forcibly kill them.
   - Flush registry.
   - Release sockets.

---

## Worker Design

### Responsibilities

- Instantiate environment via our adapter factory to guarantee consistent seeding and telemetry hooks.
- Marshal config into CleanRL invocation (either via CLI arguments or config file).
- Stream metrics:
  - Parse CleanRL stdout (JSON lines) to structured events.
  - Collect reward/loss/episode length data.
  - Emit `StepMetrics` and `EpisodeMetrics` messages.
- Register artifacts:
  - Watch output directory for checkpoints, video files, W&B run URLs.
  - Emit `CheckpointAvailable`, `VideoAvailable` events with metadata.
- Clean up temp directory (except artifacts moved into managed storage).

### Implementation Notes

- Use `asyncio.subprocess` for non-blocking reads on stdout/stderr.
- Wrap CleanRL invocation via `uv run` to respect pinned dependencies. Example command:

  ```bash
  uv run python -m cleanrl.dqn --env-id ${ENV} --total-timesteps ${TIMESTEPS} --seed ${SEED} --cfg ${CFG_PATH}
  ```

- Provide fallback to plain `python` if `uv` missing, but warn.
- Surface GPU checks before launch: if `use_gpu` flag true and `torch.cuda.is_available()` false, fail fast.
- Implement dataset verification for CORL (existence, checksum). Abort before launching expensive jobs.

---

## Event & Metrics Flow

### Event Types (`TrainerEvent`)

| Event | Payload | Consumer | Purpose |
| --- | --- | --- | --- |
| `RunStarted` | `run_id`, timestamp, config digest | GUI, CLI, Telemetry | Display status row, record start. |
| `RunStdout` | `run_id`, line | CLI `logs`, Telemetry (optional) | Live console mirroring. |
| `RunMetrics` | `run_id`, metric type, payload (`reward`, `loss`, etc.) | PyQtGraph charts, W&B adapter | Update dashboards. |
| `RunCheckpoint` | `run_id`, path, size, metric snapshot | Artifact manager | Register checkpoint in storage. |
| `RunVideo` | `run_id`, path, trigger info | Telemetry, replay UI | Link video asset to episode rollup. |
| `RunCompleted` | `run_id`, summary stats | GUI, CLI, registry | Update final status. |
| `RunFailed` | `run_id`, reason, stderr excerpt | GUI, CLI, registry | Alert user, offer diagnostics. |

### Telemetry Integration

- Extend `TelemetryService` with `training_metrics` table keyed by `(run_id, timestamp, metric_name)` storing floats or JSON payloads.
- JSONL recorder writes `TrainingMetricRecord` entries to `runtime/data/episodes/training_metrics.jsonl` for offline analysis.
- GUI Charting (`Phase 4` plan) subscribes to metrics bus and plots data in PyQtGraph.
- Exporters (TensorBoard/W&B) read from the same stream to avoid double work.

---

## Config Artifact Workflow

1. GUI collects state (`ControlPanelWidget`, hyperparameter forms, video toggles).
2. `SessionSnapshot` dataclass holds complete UI context.
3. `TrainerClient` converts snapshot → `TrainRunConfig`, adds `run_id`.
4. For export: write config to JSON file (with schema reference) so CLI/automation can reuse it.
5. CLI invocation: `gym-gui trainer submit path/to/config.json` simply loads file and forwards to daemon.

**No** separate flag parser. Schema evolution travels via versioned JSON (e.g., `"schema_version": "2025-10-14"`).

---

## Human-Mode Scripting Boundary

- Provide a **Session Harness** API for tests/automation that need to replay action sequences:

  ```python
  with SessionHarness(env_id=GameId.TAXI, seed=123, control_mode=ControlMode.HUMAN_ONLY) as session:
      session.play_actions([0, 1, 2, 3])
      session.assert_reward_gt(5)
  ```

- Harness stays entirely within the GUI session layer; it never goes through the trainer daemon, preserving semantics.
- Avoid exposing this through CLI to prevent semantic confusion (training vs. human-mode playback).

---

## Rollout Plan & Milestones

| Sprint | Deliverables | Validation |
| --- | --- | --- |
| **Week 1** | IPC scaffold, config schema, RunRegistry prototype. | Unit tests for submission validation; daemon boots & health checks pass. |
| **Week 2** | Worker executes CleanRL DQN; metrics stream; GUI receives `RunStarted/RunCompleted`. | Run a short CartPole training via GUI; ensure Qt remains responsive. |
| **Week 3** | Stop/kill commands, artifact registration, telemetry ingestion, CLI thin client. | CLI `submit` and `logs` mirror GUI events; metrics visible in PyQtGraph. |
| **Week 4** | CORL dataset preflight, GPU validation, W&B/TensorBoard hooks, retention policies. | Offline run with dataset check; W&B URL surfaced in GUI; retention job prunes old runs. |
| **Week 5 (Hardening)** | Integration tests, failure drills, documentation, ops playbook. | Simulate worker crash, GPU absence, disk full; verify graceful handling. |

---

## Pitfalls & Guardrails

1. **Config drift** – Without rigorous schema enforcement, GUI and CLI diverge. *Mitigation:* versioned JSON schema + centralized dataclasses.
2. **Zombie workers** – Orphaned trainer processes if daemon dies silently. *Mitigation:* leader election / PID files; daemon on boot kills stray workers belonging to same project root.
3. **Resource exhaustion** – Multiple GPU-hungry jobs triggered concurrently. *Mitigation:* concurrency limits based on settings; check GPU availability before launch; expose queue length in UI.
4. **Telemetry overload** – High-frequency metrics could overwhelm PyQtGraph/SQLite. *Mitigation:* throttle event emission, aggregate metrics server-side, decimate data for UI.
5. **Artifact sprawl** – Video and checkpoint files consume disk. *Mitigation:* extend StorageRecorder profiles with quotas and pruning; daemon enforces retention after run completion.
6. **Security/segfault risk** – Running arbitrary CleanRL scripts might access system resources. *Mitigation:* sanitized environment variables, optional sandbox (cgroups), log redaction.
7. **Cross-platform complexities** – Windows lacks UNIX sockets; GPU detection differs. *Mitigation:* abstract IPC layer (fallback to TCP), run specific compatibility tests.
8. **Dependency skew** – CleanRL expects Python <3.11. *Mitigation:* `uv` ensures correct interpreter; daemon verifies runtime before invoking; GUI surfaces helpful error.
9. **State reconciliation** – GUI restart while runs active. *Mitigation:* On GUI launch, query daemon for active runs, rehydrate UI state.
10. **UX mismatch** – Users expect immediate feedback after submit. *Mitigation:* emit `RunQueued` event; show queue position; notify on start.

---

## Testing & Observability Strategy

- **Unit Tests**
  - Config validation (invalid env/id/seed rejection).
  - RunRegistry transitions (submitted → running → success/fail).
  - Worker message framing (stdout -> events, metrics shape).

- **Integration Tests**
  - Launch daemon in CI, submit mock trainer (script sleeping + emitting fake metrics), assert GUI stub receives events.
  - Kill worker mid-run; ensure daemon flags failure and cleans up.
  - Submit run with invalid dataset path; expect graceful rejection.

- **End-to-End (Nightly)**
  - Run `gym-gui trainer submit` with a short CleanRL DQN config; verify checkpoint/artifacts recorded.
  - Replay metrics via CLI `logs`; compare summary to CleanRL output baseline.

- **Observability**
  - Structured logging (JSON) for daemon, worker, and client interactions.
  - Metrics counters: runs submitted, running, succeeded, failed, stop requests honored, average queue time.
  - Health endpoint: `trainer status` returns daemon PID, uptime, active runs.

---

## Documentation & Ops Playbook

- `docs/trainer_orchestration.md` detailing:
  - How to start/stop the daemon manually.
  - Config schema reference & examples.
  - CLI command usage.
  - Troubleshooting (GPU missing, dataset download failures, permission issues).
- Update Day 7 roadmap docs to reference new plan.
- Provide runbooks for cluster deployment if we later move trainer daemon off-box.

---

## Open Questions / Follow-Ups

1. **IPC choice** – Start with ZeroMQ for simplicity? Evaluate gRPC for multi-language clients later.
2. **Authentication** – Is local host communication sufficient? If multi-user scenario arises, need auth tokens.
3. **Scheduler policy** – Do we need priority queues (e.g., short vs long jobs) from day one?
4. **Multi-project isolation** – If future repo clones share the daemon, namespace runs by project root.
5. **Config evolution** – How to handle backward compatibility when introducing new hyperparameters? Possibly embed `schema_version` and migration logic.
6. **Cloud/off-box training** – Hooks should be abstract enough so backends can be swapped (Ray, Kubernetes). Keep interfaces clean.

---

## Valkka Multiprocessing Reality Check (Contrarian Addendum)

> Grounding the trainer daemon in Valkka’s Qt integration guidance forces a few uncomfortable adjustments before we write any worker code.

### 1. Boot Order Discipline vs. Our Current Bootstrap Sketch

- The Valkka notes spell it out: *start your Python multiprocesses before spinning up Qt threads, then instantiate filtergraphs, start libValkka threads, and only then hand control to the GUI loop.*
- Our present bootstrap assumes `gym_gui.app` spawns the trainer daemon lazily from inside the Qt event loop. That violates the “fork-before-threads” rule and risks the classic crashy combination Valkka warns about.
- **Contrarian adjustment:** promote the Trainer Daemon launch into the same pre-Qt bootstrap phase that already loads services. Either run it as an external long-lived process (recommended) or, if we insist on in-process spawning, do it before the first `QApplication` instance touches threads. Otherwise we inherit the same leaky behaviour Valkka sources attribute to naive forking after Qt threads exist.

### 2. MessageProcess Pattern vs. Ad-Hoc Pipes

- Valkka’s `MessageProcess` model keeps distinct *frontend* (Qt-side) and *backend* (worker-side) method namespaces mapped through a single QThread that watches pipes.
- Our daemon design leans on ZeroMQ/TCP, but the GUI still needs a thread-bridge that converts pipe events into Qt signals. The contrarian view: don’t reinvent yet another dispatcher; embrace the `MessageProcess`-style contract for GUI-facing workers and let the trainer daemon speak its own IPC dialect.
- **Actionable tension:** create a dedicated `TrainerQBridgeThread` inside the GUI that mirrors Valkka’s QThread watcher, translating daemon socket events into Qt signals without polluting the main thread. Anything else will reintroduce event-loop stalls Valkka explicitly sidesteps.

### 3. Video Surfaces and Shared Windows

- The guidance on drawing video into a widget reminds us we can either reuse existing Qt widget window IDs or embed “foreign” windows created by the streaming backend. Our orchestration plan currently punts on live video previews during training, but the telemetry pipeline expects to reason about RecordVideo outputs.
- **Contrarian hypothesis:** when we eventually graft live training previews into the GUI, we should treat the trainer daemon as the authority that owns video decoding windows (or GL contexts) and pass Qt the foreign window IDs, exactly as Valkka prescribes. Otherwise we’ll end up with two code paths: one for Valkka-style streams and another for CleanRL RecordVideo playback, fragmenting the UI layer.
- Short-term implication: reserve a slot in `TrainerEvent.RunVideo` for a `window_id` or shared memory token so we can comply with Valkka’s widget-sharing model without bolting it on later.

### 4. Multiprocess Safety Nets

- Valkka hammers on the need for a single QThread to shepherd all multiprocess pipes, warning against spawning unbounded watchers. Our daemon spec mentions “structured pipes” but leaves the supervising thread ambiguous.
- **Contrarian safeguard:** formalize a `TrainerIpcThread` (or adopt Valkka’s provided implementation) that is singular, observable, and restartable. If we scatter asyncio tasks or per-worker threads, we’ll replay the pitfalls Valkka documents (zombie subprocesses, pipe starvation).
- Moreover, Valkka’s heartbeat advice (`MessageProcess` heartbeats + restart hooks) aligns with our worker monitoring; the contrarian move is to import the same heartbeat cadence (e.g., keep-alive pings every ~1s) rather than invent a bespoke timer schedule.

### 5. C++ Escape Hatch and Future-Proofing

- The C++ API reminder is a subtle jab: once we hit performance ceilings, Valkka expects us to push heavy frame plumbing down into native threads communicating back to Python through Qt signals. Our trainer orchestrator should therefore expose hooks for native-side emitters (e.g., C++ actor controllers) without assuming every worker is a Python child process.
- **Implication:** define IPC payloads in a language-agnostic schema (Cap’n Proto, FlatBuffers, or at least JSON with explicit enums) so a future C++ Valkka worker can publish the same `TrainerEvent`s. If we weld ourselves to `msgspec`-specific Python objects now, we foreclose the upgrade path Valkka considers normal.

### 6. Fork/Thread Hazard Inventory

- Valkka’s cautionary note about “fork-combined-with-threading” pairs with our own guardrail #2, but we underplay the risk. Any time Qt starts a `QThread` (telemetry, video preview, actor services), spawning a new trainer worker from the same process could inherit partially initialized mutexes.
- **Contrarian mitigation:** restrict *all* worker spawning to either (a) an external, already-detached daemon process launched via the OS, or (b) a pre-fork pool started before Qt threads exist. The tolerable compromise is to run `TrainerDaemon` as a separate executable and let the GUI talk to it over localhost—precisely so we never fork inside the GUI process at all.

### Summary Table

| Valkka guidance | Current plan tension | Contrarian directive |
| --- | --- | --- |
| Start multiprocesses before Qt threads | GUI lazily spawns daemon after Qt boot | Promote daemon to pre-Qt bootstrap or run out-of-process |
| Single QThread bridging pipes | IPC bridge unspecified | Introduce `TrainerQBridgeThread` mirroring `MessageProcess` frontend |
| Reuse winIds / foreign widgets | Trainer events lack window handles | Bake `window_id` hooks into video events now |
| Heartbeat & restart semantics | Worker monitoring loosely defined | Adopt Valkka heartbeat cadence + restart hooks |
| C++ API symmetry | Events typed as Python-only dataclasses | Publish schema/encoding usable from C++ workers |

Taking Valkka seriously means acknowledging our architecture cannot treat the trainer daemon as “just another asyncio subprocess launcher.” We must reconcile bootstrap order, Qt thread boundaries, and IPC bridges now—otherwise the integration debt will swamp whatever ergonomics we hoped to gain from the orchestrator.

### Valkka Reference Digest

- Boot order, multiprocessing bridges, and the single QThread pattern come straight from the Valkka Qt integration notes and their “Basic organization” guidance; keep this doc open when adjusting bootstrap sequencing.[^valkka-qt]
- The PyQt testsuite install instructions explain the dependency hygiene Valkka expects (PyQt5/PySide2, `setproctitle`, media tooling); align our developer onboarding with that checklist.[^valkka-tests]
- ValkkaFS documentation outlines how the recording filesystem pre-reserves disk blocks and enforces keyframe cadence—use it when modeling our StorageRecorder retention policies.[^valkka-fs]
- Debugging playbooks (gdb workflow, shared-memory cleanup, preferred Qt binding) provide the safety nets for diagnosing trainer/Qt crashes; wire those steps into our failure drills.[^valkka-debug]
- The pitfalls compendium enumerates runtime bottlenecks (vsync, composition, NIC saturation) we should surface in ops runbooks before blaming our orchestrator.[^valkka-pitfalls]
- For deeper C++ internals and API references, consult the local `valkka-core/` checkout that ships with this workspace.

[^valkka-qt]: Valkka Examples — Integrating with Qt (local copy: `references/valkka_qt_notes.md`).
[^valkka-tests]: Valkka Examples — Installing & “Install the testsuite” section (local copy: `references/valkka_requirements.md`).
[^valkka-fs]: Valkka Examples — ValkkaFS guide (local copy: `references/valkka_valkkafs.md`).
[^valkka-debug]: Valkka Examples — Debugging guidance (local copy: `references/valkka_debugging.md`).
[^valkka-pitfalls]: Valkka Examples — Common Problems & Pitfalls (local copy: `references/valkka_pitfalls.md`).

---

## Minimal Hardening Blueprint (gRPC, ZeroMQ & Observability Constraints)

> The transport and runtime contracts now need to obey the realities laid out in `subprocess` docs, ØMQ’s reliable messaging recipes, gRPC’s built-in flow control, and the OpenTelemetry Collector pipeline.

### Transport & Flow Control

- Adopt **gRPC** for structured control channels: unary RPCs for `SubmitRun` / `CancelRun`, plus bidirectional streaming RPCs for `RunStatus` and `RunLogs`. The flow-control guide reminds us that gRPC automatically throttles writers until receivers drain their buffers—ideal for our GUI subscribers that may lag when the user is busy tweaking panels. We still surface a `max_inflight_messages` setting so the daemon can backpressure especially chatty workers.
- Retain **ZeroMQ** as an optional fast-path for intra-host metrics fan-out, but ground it in the chapter 5 patterns (pipeline + pub/sub with `zmq.PUB`/`SUB` filters). Contrarian stance: never expose raw ØMQ sockets to the GUI; instead, bridge them through the gRPC stream so flow-control semantics remain consistent across transports.
- Platform defaults flip: use TCP loopback by default (Windows friendly), enable UNIX domain sockets only when `os.name == "posix"` and permissions are hardened. Document both endpoints in the config schema.

### Daemon Lifecycle Guarantees

- Enforce singleton semantics by binding the gRPC port (or domain socket) at bootstrap and failing fast if occupied. Record PID, uptime, and build hash in a `GetHealth` unary RPC.
- Spawn every worker in a **new process group** so `TerminateRun` escalates from `SIGTERM` → timeout → `SIGKILL` without collateral damage. Maintain this policy even on Windows via `CREATE_NEW_PROCESS_GROUP`.

### Subprocess Discipline (Python `subprocess` Canon)

- Launch workers with `asyncio.create_subprocess_exec`, mirroring the Python docs’ admonition to continuously drain both `stdout` and `stderr` pipes to avoid deadlocks. We’ll clamp log emission to N lines/sec per stream, dropping excess with a counter-based “suppressed lines” notice to keep GUI logs readable.
- Environment sanitization: inherit only whitelisted vars (PATH, VIRTUAL_ENV) and inject our configuration via command-line or env overrides, preventing unbounded user overrides from leaking into daemon internals.

### Registry & Idempotency

- Keep the SQLite registry in WAL mode with short, parameterized transactions; schedule automatic checkpoints every ~1k frames to avoid unbounded WAL growth. Apply unique constraints on `(run_id)` and `(config_digest)` so resubmits either fast-forward or reject duplicates with a clear `ALREADY_EXISTS` error.
- No BLOB storage—artifacts stay on disk and are referenced by path & checksum.

### Telemetry & Observability

- Emit metrics and traces as OTLP over gRPC directly to a locally hosted **OpenTelemetry Collector**, per Better Stack’s deployment guide. The collector then fans out to SQLite, JSONL, W&B, or anything ops wants—our daemon simply exports standardized spans (`trainer.run`, `trainer.worker.spawn`, `trainer.event.write`).
- The GUI/CLI clients subscribe to the daemon’s gRPC streams for live updates but rely on the collector (or telemetry SQLite) for historical queries, eliminating bespoke exporters.

### Schema Evolution Discipline

- Ship every `TrainRunConfig` alongside a JSON Schema annotated with compatibility mode (`BACKWARD` vs `FULL`). CI must replay the previous release’s configs (N-1) through the daemon and assert acceptance, or fail with explicit migration notes.
- Version the schema via `schema_version` in the config payload; the daemon refuses future-major versions to avoid undefined behaviour.

### Platform Coverage

- Bake cross-platform tests that exercise loopback TCP, Windows process groups, and POSIX domain sockets. Only advertise the UNIX endpoint when integration tests pass.
- Document all behavioural toggles (`use_unix_socket`, `max_inflight_messages`, `otlp_endpoint`) so downstream automation can configure them without spelunking into source.

This hardening pass locks transport semantics, subprocess hygiene, persistence guarantees, and observability into place before any GUI niceties. Anything less ignores the lessons from the gRPC flow-control guide, ZeroMQ’s resilient pipelines, Python’s subprocess pitfalls, and the OpenTelemetry collector’s role in a real monitoring stack.

---

## Operational Guardrails (Source-Backed)

- **Never block the GUI thread.** All daemon IPC listeners must live in a dedicated bridge thread that relays events into Qt via queued signals; direct socket reads inside the GUI event loop violate Qt’s threading contract and risk deadlocks.[^qt-thread]
- **Keep the control plane binary-clean.** Only structured control messages traverse the gRPC/ZeroMQ channels; raw video frames or large binary blobs stay on the artifact bus so Unix-domain socket buffers never saturate and backpressure remains predictable.[^unix-buffer]
- **Practice WAL hygiene.** The run registry operates in SQLite WAL mode, but a background checkpoint must fire before the WAL crosses the auto-checkpoint threshold or when the daemon idles, preventing unbounded growth and amortizing fsync costs.[^sqlite-wal]
- **Make subscribers whole.** Metrics fan-out honors the ZeroMQ XPUB/XSUB replay pattern: every update carries a monotonic sequence, new or recovering subscribers fetch a snapshot first, and the daemon replays gaps on demand instead of dropping frames silently.[^zeromq-guard]

See `/references/` for offline copies of the cited material.

[^qt-thread]: Qt Documentation — QThread Class (Local copy: `references/qt_qthread_doc.md`).
[^unix-buffer]: Linux man-pages Project — `unix(7)` Socket Behavior (Local copy: `references/man7_unix7.md`).
[^sqlite-wal]: SQLite Write-Ahead Logging Checkpointing Notes (Local copy: `references/sqlite_wal_checkpointing.md`).
[^zeromq-guard]: "ZeroMQ: Messaging for Many Applications", Chapter 5 – Advanced Pub-Sub Patterns (Local copy: `references/zeromq_chapter5_pub_sub.md`).

---

## Contrarian Verdict

- **Do not** build a CLI that mirrors GUI toggles directly; it doubles maintenance and perpetuates the same single-process constraint.
- **Do** invest in the trainer orchestrator now; every other feature (CLI automation, replay integration, agent benchmarking) hangs off this backbone.
- **Measure success** by UI latency remaining stable during long runs, telemetry remaining consistent, and automation consuming the same APIs as humans.

By sequencing work around the orchestrator, we solve the foundational problems once, avoid two divergent configuration surfaces, and unlock a sustainable path for agent training, offline experimentation, and future cloud backends.
