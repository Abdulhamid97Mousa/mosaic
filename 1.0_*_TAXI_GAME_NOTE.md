# Taxi Game Note

## Scenario snapshot

- **Map:** Fixed 5×5 grid with four depots labelled `R`, `G`, `Y`, `B`.
- **Objective:** Navigate the taxi to the passenger, pick them up, deliver to the requested depot.
- **Action space:** Discrete(6) → `0: south`, `1: north`, `2: east`, `3: west`, `4: pickup`, `5: dropoff`.
- **Observation space:** Discrete(500) encoding `(taxi_row, taxi_col, passenger_location, destination)`; 404 of those states are reachable in practice.
- **Reward schedule:** `-1` per time step, `+20` for a correct drop-off, `-10` for illegal pickup/drop-off attempts.

## Dynamics at a glance

- Walls embedded in the map block lateral moves and generate a noop (still costs -1).
- `info["action_mask"]` identifies which actions are currently legal; sampling from it avoids noops.
- **Movement is deterministic** in the current Gymnasium Taxi-v3 implementation.
- The optional stochastic variants (`is_raining`, `fickle_passenger`) mentioned in older documentation are **not supported** in modern Gymnasium versions.

## Mermaid overview of a typical delivery

```mermaid
stateDiagram-v2
    [*] --> Search
    Search --> PickupReady: taxi aligns with passenger tile
    PickupReady --> Pickup: action 4
    Pickup --> Navigate: passenger onboard
    Navigate --> DropReady: taxi reaches destination depot
    DropReady --> Dropoff: action 5 (reward +20)
    Dropoff --> [*]
    Search --> IllegalPickup: action 4 without passenger
    IllegalPickup --> Search: penalty -10
    Navigate --> WallHit: move blocked → noop
    WallHit --> Navigate
```

## Training tips

1. **Baseline:** `gym.make("Taxi-v3")` with default deterministic dynamics.
2. **Exploration:** Use epsilon-greedy strategies or action masking (`info["action_mask"]`) to avoid illegal actions that incur -10 penalties.
3. **Curriculum:** Start with simple policies (e.g., shortest path to passenger, then to destination) before training more sophisticated hierarchical or deep RL approaches.

## References

- Gymnasium documentation – [Taxi](https://gymnasium.farama.org/environments/toy_text/taxi/).
- Dietterich, T. “Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition,” JAIR, 2000.
